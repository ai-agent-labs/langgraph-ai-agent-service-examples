# Ch3: LangChain ê¸°ì´ˆ - Q&A ì±—ë´‡

LangChain LCEL(LangChain Expression Language)ì„ í™œìš©í•œ ê¸°ë³¸ ì§ˆì˜ì‘ë‹µ ì±—ë´‡ ì˜ˆì œì…ë‹ˆë‹¤.

## ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥

- âœ… **LCEL íŒŒì´í”„ë¼ì¸**: `prompt | model | output_parser` êµ¬ì¡°
- âœ… **ìµœì‹  LangChain 1.1.x**: `init_chat_model` ì‚¬ìš©
- âœ… **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**: `.stream()` ë©”ì„œë“œë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ
- âœ… **ê°„ë‹¨í•œ UI**: Streamlit ê¸°ë°˜ ì›¹ ì¸í„°í˜ì´ìŠ¤

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# uv ì‚¬ìš© (ê¶Œì¥)
uv sync

# ë˜ëŠ” pip ì‚¬ìš©
pip install -e .
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
cp .env.example .env
# .env íŒŒì¼ì„ ì—´ì–´ OPENAI_API_KEY ì„¤ì •
```

### 3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

```bash
# Streamlit ì•± ì‹¤í–‰
uv run streamlit run app.py

# ë˜ëŠ”
streamlit run app.py
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ch03-langchain-basics/
â”œâ”€â”€ src/qa_chatbot/
â”‚   â”œâ”€â”€ __init__.py      # íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
â”‚   â”œâ”€â”€ config.py        # ì„¤ì • ê´€ë¦¬ (pydantic-settings)
â”‚   â”œâ”€â”€ prompts.py       # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
â”‚   â””â”€â”€ chain.py         # LCEL ì²´ì¸ êµ¬í˜„
â”œâ”€â”€ app.py               # Streamlit ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
â”œâ”€â”€ pyproject.toml       # í”„ë¡œì íŠ¸ ì„¤ì •
â”œâ”€â”€ .env.example         # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì œ
â””â”€â”€ README.md
```

## ğŸ’¡ í•µì‹¬ ì½”ë“œ íŒ¨í„´

### 1. ê¸°ë³¸ LCEL ì²´ì¸ êµ¬ì„±

```python
from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

model = init_chat_model("gpt-5.2", model_provider="openai")

prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ Q&A ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
    ("user", "{question}")
])

chain = prompt | model | StrOutputParser()

result = chain.invoke({"question": "Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"})
```

### 2. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
for chunk in chain.stream({"question": "AIì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"}):
    print(chunk, end="", flush=True)
```

### 3. RunnableLambdaë¥¼ í™œìš©í•œ í›„ì²˜ë¦¬

```python
from langchain_core.runnables import RunnableLambda

def add_ai_indicator(message):
    message.content += "\n\n(ì´ ì‘ë‹µì€ AIì— ì˜í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.)"
    return message

chain = prompt | model | RunnableLambda(add_ai_indicator) | StrOutputParser()
```

### 4. RunnableParallelì„ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬

```python
from langchain_core.runnables import RunnableParallel
from langchain_core.prompts import PromptTemplate

pros_chain = PromptTemplate.from_template("{subject}ì˜ ì¥ì ") | model
cons_chain = PromptTemplate.from_template("{subject}ì˜ ë‹¨ì ") | model

parallel_chain = RunnableParallel(pros=pros_chain, cons=cons_chain)
result = parallel_chain.invoke({"subject": "AI"})
```

## ğŸ”— ê´€ë ¨ ì±•í„°

- **Ch6: ë©”ëª¨ë¦¬** - ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ë¡œ í™•ì¥
- **Ch7: ë„êµ¬/MCP** - ì™¸ë¶€ ë„êµ¬ í†µí•©
- **Ch8: êµ¬ì¡°í™”ëœ ì¶œë ¥** - Pydantic ëª¨ë¸ë¡œ ì¶œë ¥ êµ¬ì¡°í™”

## ğŸ“š ì°¸ê³  ìë£Œ

- [LangChain LCEL ê³µì‹ ë¬¸ì„œ](https://python.langchain.com/docs/expression_language/)
- [init_chat_model ê°€ì´ë“œ](https://python.langchain.com/docs/how_to/chat_models_universal_init/)
- [Streamlit ë¬¸ì„œ](https://docs.streamlit.io/)

## âš™ï¸ ê°œë°œ ë„êµ¬

```bash
# ì½”ë“œ í¬ë§·íŒ…
uv run ruff format .

# ë¦°íŒ…
uv run ruff check .

# íƒ€ì… ì²´í¬
uv run mypy src/
```

## ğŸ“ ë¼ì´ì„ ìŠ¤

MIT License
